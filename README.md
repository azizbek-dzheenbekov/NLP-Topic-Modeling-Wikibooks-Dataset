# NLP - Topic modeling with Wikibooks dataset
This Jupyter notebook demonstrates the use of various topic modeling techniques on the Wikibooks dataset. The goal is to suggest categories for a new book store and categorize books based on their content.

## Dataset Description
The Wikibooks dataset consists of contents from all Wikibooks in 12 languages, including English, French, German, Spanish, Portuguese, Italian, Russian, Japanese, Dutch, Polish, Hungarian, and Hebrew. The dataset is divided into chapters, each with its own webpage. The content is provided in both a new line delimited textual format and its HTML for better semantic parsing.

## Technologies Used
The notebook uses various NLP techniques such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), Ida2Vec, and CorEx for topic modeling. These techniques are implemented using Python libraries such as Gensim and Corex.

## Contents
- Loading and preprocessing the dataset
- Exploratory Data Analysis (EDA) of the dataset
- Topic modeling using LDA, LSA, Ida2Vec, and CorEx
- Visualization of the topics and their distribution

This notebook is suitable for anyone interested in learning about topic modeling and NLP techniques, as well as how to apply them to real-world datasets like the Wikibooks dataset.
